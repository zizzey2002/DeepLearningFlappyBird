#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Flappy Bird Double DQN 2025 ç°ä»£ä¿®å¤ç‰ˆ

ç‰¹ç‚¹ï¼š
- TensorFlow 1.x é£æ ¼ï¼ˆtf.compat.v1ï¼‰
- ä½¿ç”¨ Double DQNï¼ˆä¸»ç½‘é€‰åŠ¨ä½œ + ç›®æ ‡ç½‘ä¼°å€¼ï¼‰
- ä½¿ç”¨ target network æå‡ç¨³å®šæ€§
- ç®€å• reward shapingï¼šå­˜æ´»+å¾®å¼±å¥–åŠ±ï¼Œè¿‡ç®¡å­+1ï¼Œæ­»äº¡-1
"""

from __future__ import print_function
import tensorflow as tf
tf.compat.v1.disable_eager_execution()

import cv2
import sys
import random
import numpy as np
from collections import deque

sys.path.append("game/")
import wrapped_flappy_bird as game

# ==================== è¶…å‚æ•° ====================
GAME = 'bird'
ACTIONS = 2                 # åŠ¨ä½œæ•°ï¼šä¸è·³ / è·³
GAMMA = 0.99                # æŠ˜æ‰£å› å­
OBSERVE = 1000              # çº¯è§‚å¯Ÿæ­¥æ•°ï¼ˆåªæ”¶é›†ç»éªŒï¼Œä¸è®­ç»ƒï¼‰
EXPLORE = 300000            # epsilon ä» INITIAL è¡°å‡åˆ° FINAL æ‰€èŠ±çš„æ­¥æ•°
INITIAL_EPSILON = 0.5
FINAL_EPSILON = 0.05
REPLAY_MEMORY = 100000      # ç»éªŒæ± å®¹é‡
BATCH = 32
FRAME_PER_ACTION = 4
LEARNING_RATE = 2.5e-4      # ç¨å¾®è°ƒé«˜ä¸€ç‚¹ï¼ŒAdam æ¯”è¾ƒç¨³
TARGET_UPDATE_FREQ = 1000   # target ç½‘ç»œåŒæ­¥é¢‘ç‡
SAVE_INTERVAL = 20000       # ä¿å­˜æ¨¡å‹æ­¥æ•°é—´éš”

# ==================== ç½‘ç»œæƒé‡è¾…åŠ©å‡½æ•° ====================
def weight_variable(shape):
    initial = tf.compat.v1.truncated_normal(shape, stddev=0.01)
    return tf.compat.v1.Variable(initial)

def bias_variable(shape):
    initial = tf.constant(0.01, shape=shape)
    return tf.compat.v1.Variable(initial)

def conv2d(x, W, stride):
    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding="SAME")

def max_pool_2x2(x):
    return tf.nn.max_pool(
        x,
        ksize=[1, 2, 2, 1],
        strides=[1, 2, 2, 1],
        padding="SAME"
    )

# ==================== ç½‘ç»œç»“æ„ï¼ˆä¸»ç½‘ç»œ / ç›®æ ‡ç½‘ç»œå…±ç”¨ï¼‰ ====================
def createNetwork(name='q_network'):
    with tf.compat.v1.variable_scope(name):
        # å·ç§¯å±‚
        W_conv1 = weight_variable([8, 8, 4, 32])
        b_conv1 = bias_variable([32])

        W_conv2 = weight_variable([4, 4, 32, 64])
        b_conv2 = bias_variable([64])

        W_conv3 = weight_variable([3, 3, 64, 64])
        b_conv3 = bias_variable([64])

        # å…¨è¿æ¥å±‚
        W_fc1 = weight_variable([1600, 512])
        b_fc1 = bias_variable([512])

        W_fc2 = weight_variable([512, ACTIONS])
        b_fc2 = bias_variable([ACTIONS])

        # è¾“å…¥ï¼š80x80ï¼Œ4 å¸§å †å 
        s = tf.compat.v1.placeholder("float", [None, 80, 80, 4])

        h_conv1 = tf.nn.relu(conv2d(s, W_conv1, 4) + b_conv1)
        h_pool1 = max_pool_2x2(h_conv1)

        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2, 2) + b_conv2)
        h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 1) + b_conv3)

        h_conv3_flat = tf.reshape(h_conv3, [-1, 1600])
        h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat, W_fc1) + b_fc1)

        # è¾“å‡ºï¼šæ¯ä¸ªåŠ¨ä½œçš„ Q å€¼
        readout = tf.matmul(h_fc1, W_fc2) + b_fc2

    return s, readout, h_fc1

# ==================== ä¸»è®­ç»ƒå‡½æ•°ï¼ˆDouble DQNï¼‰ ====================
def trainNetwork(s, readout, h_fc1, sess):
    # ----- 1. åˆ›å»ºç›®æ ‡ç½‘ç»œ -----
    s_target, readout_target, h_fc1_target = createNetwork(name='target_network')

    # åŠ¨ä½œ one-hot & Q ç›®æ ‡å€¼
    a = tf.compat.v1.placeholder("float", [None, ACTIONS])
    y = tf.compat.v1.placeholder("float", [None])

    # é€‰ä¸­å½“å‰åŠ¨ä½œå¯¹åº”çš„ Q(s,a)
    readout_action = tf.reduce_sum(tf.multiply(readout, a), axis=1)

    # ä½¿ç”¨ Huber loss ç¨å¾®ç¨³ä¸€ç‚¹ï¼ˆå¯é€‰ï¼‰ï¼Œä¹Ÿå¯ä»¥ç»§ç»­ç”¨ MSE
    # cost = tf.reduce_mean(tf.square(y - readout_action))
    cost = tf.compat.v1.losses.huber_loss(y, readout_action)

    optimizer = tf.compat.v1.train.AdamOptimizer(LEARNING_RATE)
    train_step = optimizer.minimize(cost)

    # ----- 2. æ„å»º target network å‚æ•°åŒæ­¥æ“ä½œ -----
    # ç°åœ¨æˆ‘ä»¬ç”¨ variable_scopeï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥æŒ‰ scope æ‹¿å˜é‡ï¼ˆæ›´å®‰å…¨ï¼‰
    main_vars   = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES,
                                              scope='q_network')
    target_vars = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES,
                                              scope='target_network')

    copy_target_op = [target_vars[i].assign(main_vars[i]) for i in range(len(main_vars))]

    # ----- 3. åˆå§‹åŒ–æ¸¸æˆç¯å¢ƒå’Œç»éªŒæ±  -----
    game_state = game.GameState()
    D = deque(maxlen=REPLAY_MEMORY)

    # è·å¾—åˆå§‹çŠ¶æ€ï¼šå…ˆæ‰§è¡Œä¸€æ¬¡â€œdo nothingâ€
    do_nothing = np.zeros(ACTIONS)
    do_nothing[0] = 1
    x_t, r_0, terminal = game_state.frame_step(do_nothing)

    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)
    _, x_t = cv2.threshold(x_t, 128, 255, cv2.THRESH_BINARY)  # é˜ˆå€¼ç”¨ 128ï¼Œæ›´ç¨³å®š
    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)              # å †å æˆ 4 å¸§

    # Saver
    saver = tf.compat.v1.train.Saver()

    # # ==== åŠ è½½å·²æœ‰æ¨¡å‹ç»§ç»­è®­ç»ƒï¼ˆå¯é€‰ï¼‰====
    # checkpoint_path = "saved_networks/bird-dqn-60000"
    # saver.restore(sess, checkpoint_path)
    # print("æˆåŠŸä» 60000 æ­¥çš„æ¨¡å‹ç»§ç»­è®­ç»ƒï¼")

    # ----- 4. åˆå§‹åŒ–å˜é‡å¹¶åŒæ­¥ä¸€æ¬¡ target ç½‘ç»œ -----
    sess.run(tf.compat.v1.global_variables_initializer())
    sess.run(copy_target_op)
    print("å¼€å§‹è®­ç»ƒï¼å°é¸Ÿè¦èµ·é£äº†ğŸš€ï¼ˆDouble DQN + target network å·²åˆå§‹åŒ–ï¼‰")

    epsilon = INITIAL_EPSILON
    t = 0

    # ==================== è®­ç»ƒä¸»å¾ªç¯ ====================
    while True:
        # 1) ä½¿ç”¨ Îµ-greedy é€‰æ‹©åŠ¨ä½œï¼ˆç”¨ä¸»ç½‘ç»œï¼‰
        readout_t = sess.run(readout, feed_dict={s: [s_t]})[0]
        a_t = np.zeros([ACTIONS])
        action_index = 0

        if t % FRAME_PER_ACTION == 0:
            if random.random() <= epsilon:
                # é™ä½éšæœºè·³è·ƒæ¦‚ç‡ï¼ˆä¾‹å¦‚ 10% éšæœºè·³ï¼Œ90% éšæœºä¸è·³ï¼‰
                jump_random_prob = 0.10
                if random.random() < jump_random_prob:
                    action_index = 1   # éšæœºè·³
                else:
                    action_index = 0   # éšæœºä¸è·³
                a_t[action_index] = 1
            else:
                action_index = np.argmax(readout_t)
                a_t[action_index] = 1
        else:
            # ä¸åœ¨åŠ¨ä½œå¸§å°±ä¸è·³
            a_t[0] = 1

        # Îµ çº¿æ€§é€€ç«
        if t > OBSERVE and epsilon > FINAL_EPSILON:
            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / float(EXPLORE)
            epsilon = max(FINAL_EPSILON, epsilon)

        # 2) æ‰§è¡ŒåŠ¨ä½œï¼Œè·å¾—æ–°å¸§ã€åŸå§‹ rewardã€æ˜¯å¦æ­»äº¡
        x_t1_colored, r_t, terminal = game_state.frame_step(a_t)

        # å¥–åŠ±æ•´å½¢
        if terminal:
            r_t = -1.0              # æ­»äº¡æƒ©ç½š
        elif r_t == 1:              # åªæœ‰è¿‡ç®¡å­æ‰æ˜¯ 1
            r_t = 1.0
        else:
            r_t = 0.0005            # æ´»ç€

        # 3) é¢„å¤„ç†ä¸‹ä¸€å¸§
        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)
        _, x_t1 = cv2.threshold(x_t1, 128, 255, cv2.THRESH_BINARY)
        x_t1 = np.reshape(x_t1, (80, 80, 1))
        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)   # æ–°å¸§åŠ åˆ°æœ€å‰é¢

        # 4) å­˜å…¥ç»éªŒæ± 
        D.append((s_t, a_t, r_t, s_t1, terminal))
        s_t = s_t1
        t += 1

        # 5) ä»ç»éªŒæ± é‡‡æ ·è®­ç»ƒï¼ˆè¶…è¿‡ OBSERVE æ‰å¼€å§‹ï¼‰
        if t > OBSERVE:
            minibatch = random.sample(D, BATCH)

            s_j_batch   = [d[0] for d in minibatch]
            a_batch     = [d[1] for d in minibatch]
            r_batch     = [d[2] for d in minibatch]
            s_j1_batch  = [d[3] for d in minibatch]
            terminal_batch = [d[4] for d in minibatch]

            # ===== Double DQN æ ¸å¿ƒéƒ¨åˆ† =====
            # â‘  ç”¨ä¸»ç½‘ç»œåœ¨ s' ä¸Šé€‰åŠ¨ä½œï¼ˆargmaxï¼‰
            q_next_main = sess.run(readout, feed_dict={s: s_j1_batch})
            # â‘¡ ç”¨ç›®æ ‡ç½‘ç»œåœ¨ s' ä¸Šè¯„ä¼°è¿™äº›åŠ¨ä½œçš„ Q å€¼
            q_next_target = sess.run(readout_target, feed_dict={s_target: s_j1_batch})

            y_batch = []
            for i in range(len(minibatch)):
                if terminal_batch[i]:
                    # ç»ˆæ­¢çŠ¶æ€ï¼šæ²¡æœ‰æœªæ¥å›æŠ¥
                    y_batch.append(r_batch[i])
                else:
                    # ä¸»ç½‘ç»œé€‰ a_max
                    a_max = np.argmax(q_next_main[i])
                    # ç›®æ ‡ç½‘ç»œç»™å‡ºè¯¥åŠ¨ä½œä»·å€¼
                    target_q = q_next_target[i][a_max]
                    y_batch.append(r_batch[i] + GAMMA * target_q)

            # æ¢¯åº¦æ›´æ–°ä¸€æ­¥ï¼ˆåªæ›´æ–°ä¸»ç½‘ç»œå‚æ•°ï¼‰
            sess.run(train_step, feed_dict={
                y:  y_batch,
                a:  a_batch,
                s:  s_j_batch
            })

        # 6) å®šæœŸåŒæ­¥ target ç½‘ç»œï¼ˆå¤åˆ¶ä¸»ç½‘ç»œå‚æ•°ï¼‰
        if t % TARGET_UPDATE_FREQ == 0:
            sess.run(copy_target_op)
            print("Target network updated at step", t)

        # 7) å®šæœŸä¿å­˜æ¨¡å‹
        if t % SAVE_INTERVAL == 0:
            saver.save(sess, 'saved_networks/' + GAME + '-double-dqn', global_step=t)
            print("ç¬¬ {} æ­¥ - æ¨¡å‹å·²ä¿å­˜ï¼å½“å‰ Îµ = {:.3f}".format(t, epsilon))

        # 8) æ‰“å°è®­ç»ƒçŠ¶æ€
        if t <= OBSERVE:
            state = "observe"
        elif t <= OBSERVE + EXPLORE:
            state = "explore"
        else:
            state = "train"

        print("Step {} | {} | Îµ = {:.3f} | Action = {} | Reward = {:.4f}".format(
            t, state, epsilon, action_index, r_t
        ))

# ==================== å…¥å£å‡½æ•° ====================
def playGame():
    sess = tf.compat.v1.InteractiveSession()
    # ä¸»ç½‘ç»œç”¨ä¸“é—¨çš„ scopeï¼Œæ–¹ä¾¿å’Œ target ç½‘ç»œåŒºåˆ†
    s, readout, h_fc1 = createNetwork(name='q_network')
    trainNetwork(s, readout, h_fc1, sess)

if __name__ == "__main__":
    playGame()